{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh0w6Isp26ay"
      },
      "source": [
        "# Trains a diffusion model on CIFAR-10 (version 2).\n",
        "\n",
        "By Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings).\n",
        "\n",
        "The model is a denoising diffusion probabilistic model (https://arxiv.org/abs/2006.11239), which is trained to reverse a gradual noising process, allowing the model to generate samples from the learned data distribution starting from random noise. DDIM-style deterministic sampling (https://arxiv.org/abs/2010.02502) is also supported. This model is also trained on continuous timesteps parameterized by the log SNR on each timestep (see Variational Diffusion Models, https://arxiv.org/abs/2107.00630), allowing different noise schedules than the one used during training to be easily used during sampling. It uses the 'v' objective from Progressive Distillation for Fast Sampling of Diffusion Models (https://openreview.net/forum?id=TIdIXIpzhoI) for better conditioned denoised images at high noise levels, but reweights the loss function so that it has the same relative weighting as the 'eps' objective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAUwPLG92r89",
        "cellView": "form"
      },
      "source": [
        "# @title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M99bmqIPyw_Q"
      },
      "source": [
        "# Check the GPU type\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w5A9GHfynNT"
      },
      "source": [
        "# Imports\n",
        "from contextlib import contextmanager\n",
        "from copy import deepcopy\n",
        "import math\n",
        "import numpy as np\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm, trange"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8IFYM8fy5h8"
      },
      "source": [
        "# Utilities\n",
        "\n",
        "@contextmanager\n",
        "def train_mode(model, mode=True):\n",
        "    \"\"\"A context manager that places a model into training mode and restores\n",
        "    the previous mode on exit.\"\"\"\n",
        "    modes = [module.training for module in model.modules()]\n",
        "    try:\n",
        "        yield model.train(mode)\n",
        "    finally:\n",
        "        for i, module in enumerate(model.modules()):\n",
        "            module.training = modes[i]\n",
        "def eval_mode(model):\n",
        "    \"\"\"A context manager that places a model into evaluation mode and restores\n",
        "    the previous mode on exit.\"\"\"\n",
        "    return train_mode(model, False)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def ema_update(model, averaged_model, decay):\n",
        "    \"\"\"Incorporates updated model parameters into an exponential moving averaged\n",
        "    version of a model. It should be called after each optimizer step.\"\"\"\n",
        "    model_params = dict(model.named_parameters())\n",
        "    averaged_params = dict(averaged_model.named_parameters())\n",
        "    assert model_params.keys() == averaged_params.keys()\n",
        "\n",
        "    for name, param in model_params.items():\n",
        "        averaged_params[name].mul_(decay).add_(param, alpha=1 - decay)\n",
        "\n",
        "    model_buffers = dict(model.named_buffers())\n",
        "    averaged_buffers = dict(averaged_model.named_buffers())\n",
        "    assert model_buffers.keys() == averaged_buffers.keys()\n",
        "\n",
        "    for name, buf in model_buffers.items():\n",
        "        averaged_buffers[name].copy_(buf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DR14Jaly8FZ"
      },
      "source": [
        "# Define the model (a residual U-Net)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input) + self.skip(input)\n",
        "\n",
        "\n",
        "class ResConvBlock(ResidualBlock):\n",
        "    def __init__(self, c_in, c_mid, c_out, dropout_last=True):\n",
        "        skip = None if c_in == c_out else nn.Conv2d(c_in, c_out, 1, bias=False)\n",
        "        super().__init__([\n",
        "            nn.Conv2d(c_in, c_mid, 3, padding=1),\n",
        "            nn.Dropout2d(0.1, inplace=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(c_mid, c_out, 3, padding=1),\n",
        "            nn.Dropout2d(0.1, inplace=True) if dropout_last else nn.Identity(),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ], skip)\n",
        "\n",
        "\n",
        "class SkipBlock(nn.Module):\n",
        "    def __init__(self, main, skip=None):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(*main)\n",
        "        self.skip = skip if skip else nn.Identity()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.cat([self.main(input), self.skip(input)], dim=1)\n",
        "\n",
        "\n",
        "class FourierFeatures(nn.Module):\n",
        "    def __init__(self, in_features, out_features, std=1.):\n",
        "        super().__init__()\n",
        "        assert out_features % 2 == 0\n",
        "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
        "\n",
        "    def forward(self, input):\n",
        "        f = 2 * math.pi * input @ self.weight.T\n",
        "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
        "\n",
        "\n",
        "def expand_to_planes(input, shape):\n",
        "    return input[..., None, None].repeat([1, 1, shape[2], shape[3]])\n",
        "\n",
        "\n",
        "class Diffusion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        c = 64  # The base channel count\n",
        "\n",
        "        # The inputs to timestep_embed will approximately fall into the range\n",
        "        # -10 to 10, so use std 0.2 for the Fourier Features.\n",
        "        self.timestep_embed = FourierFeatures(1, 16, std=0.2)\n",
        "        #self.class_embed = nn.Embedding(10, 4)\n",
        "\n",
        "        self.net = nn.Sequential(   # 32x32\n",
        "            ResConvBlock(3 + 16 + 4, c, c),\n",
        "            ResConvBlock(c, c, c),\n",
        "            SkipBlock([\n",
        "                nn.AvgPool2d(2),  # 32x32 -> 16x16\n",
        "                ResConvBlock(c, c * 2, c * 2),\n",
        "                ResConvBlock(c * 2, c * 2, c * 2),\n",
        "                SkipBlock([\n",
        "                    nn.AvgPool2d(2),  # 16x16 -> 8x8\n",
        "                    ResConvBlock(c * 2, c * 4, c * 4),\n",
        "                    ResConvBlock(c * 4, c * 4, c * 4),\n",
        "                    SkipBlock([\n",
        "                        nn.AvgPool2d(2),  # 8x8 -> 4x4\n",
        "                        ResConvBlock(c * 4, c * 8, c * 8),\n",
        "                        ResConvBlock(c * 8, c * 8, c * 8),\n",
        "                        ResConvBlock(c * 8, c * 8, c * 8),\n",
        "                        ResConvBlock(c * 8, c * 8, c * 4),\n",
        "                        nn.Upsample(scale_factor=2),\n",
        "                    ]),  # 4x4 -> 8x8\n",
        "                    ResConvBlock(c * 8, c * 4, c * 4),\n",
        "                    ResConvBlock(c * 4, c * 4, c * 2),\n",
        "                    nn.Upsample(scale_factor=2),\n",
        "                ]),  # 8x8 -> 16x16\n",
        "                ResConvBlock(c * 4, c * 2, c * 2),\n",
        "                ResConvBlock(c * 2, c * 2, c),\n",
        "                nn.Upsample(scale_factor=2),\n",
        "            ]),  # 16x16 -> 32x32\n",
        "            ResConvBlock(c * 2, c, c),\n",
        "            ResConvBlock(c, c, 3, dropout_last=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, log_snrs, cond):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(log_snrs[:, None]), input.shape)\n",
        "        #class_embed = expand_to_planes(self.class_embed(cond), input.shape)\n",
        "        #print(class_embed.shape)\n",
        "        b,c,h,w = input.shape\n",
        "        class_embed = torch.zeros(b,4,h,w).to(device)\n",
        "        #class_embed = torch.zeros_like(input)[:, :4].to(device)\n",
        "        #print(class_embed.shape)\n",
        "        return self.net(torch.cat([input, class_embed, timestep_embed], dim=1))\n",
        "    def get_features(self, input, log_snrs, cond):\n",
        "        timestep_embed = expand_to_planes(self.timestep_embed(log_snrs[:, None]), input.shape)\n",
        "        #class_embed = expand_to_planes(self.class_embed(cond), input.shape)\n",
        "        b,c,h,w = input.shape\n",
        "        class_embed = torch.zeros(b,4,h,w).to(device)\n",
        "        x = torch.cat([input, class_embed, timestep_embed], dim=1)\n",
        "\n",
        "        features = []\n",
        "        features_before_up = []\n",
        "        res_bl_lvl1_num = 0\n",
        "        for module in self.net:\n",
        "            if isinstance(module, ResConvBlock):\n",
        "                x = module(x)\n",
        "                features.append(x)\n",
        "                #print(\"Res detected on level1\")\n",
        "                res_bl_lvl1_num += 1\n",
        "                if res_bl_lvl1_num == 3:\n",
        "                    features_before_up.append(x)\n",
        "\n",
        "            if isinstance(module, SkipBlock):\n",
        "                #print(\"Skipblock level1\")\n",
        "                before_skip1 = x\n",
        "                for module1 in module.main:\n",
        "                    if isinstance(module1, nn.AvgPool2d):\n",
        "                        #print(\"AvgPool2d detected level2\")\n",
        "\n",
        "                        x = module1(x)\n",
        "                    if isinstance(module1, ResConvBlock):\n",
        "                        x = module1(x)\n",
        "                        features.append(x)\n",
        "                        #print(\"Res detected level2\")\n",
        "\n",
        "                    if isinstance(module1, SkipBlock):\n",
        "                        #print(\"Skipblock level2\")\n",
        "                        before_skip2 = x\n",
        "                        for module2 in module1.main:\n",
        "                            if isinstance(module2, nn.AvgPool2d):\n",
        "                                x = module2(x)\n",
        "                                #print(\"AvgPool2d detected level3\")\n",
        "\n",
        "                            if isinstance(module2, ResConvBlock):\n",
        "                                x = module2(x)\n",
        "                                features.append(x)\n",
        "                                #print(\"Res detected level3\")\n",
        "\n",
        "                            if isinstance(module2, SkipBlock):\n",
        "                                #print(\"Skipblock level3\")\n",
        "                                before_skip3 = x\n",
        "                                for module3 in module2.main:\n",
        "                                    if isinstance(module3, nn.AvgPool2d):\n",
        "                                        #print(\"AvgPool2d detected level4\")\n",
        "                                        x = module3(x)\n",
        "                                    if isinstance(module3, ResConvBlock):\n",
        "                                        x = module3(x)\n",
        "                                        features.append(x)\n",
        "                                        #print(\"Res detected level4\")\n",
        "\n",
        "                                    if isinstance(module3, nn.Upsample):\n",
        "                                        #print(\"upsample before: \", x.shape)\n",
        "                                        features_before_up.append(x)\n",
        "\n",
        "                                        #print(\"Up detected level4\")\n",
        "                                        x = module3(x)\n",
        "                                        x = torch.cat([x, before_skip3], dim=1)\n",
        "\n",
        "                                        #print(\"upsample after: \", x.shape)\n",
        "                            if isinstance(module2, nn.Upsample):\n",
        "                                features_before_up.append(x)\n",
        "                                #print(\"Up detected level3\")\n",
        "                                x = module2(x)\n",
        "                                x = torch.cat([x, before_skip2], dim=1)\n",
        "                    if isinstance(module1, nn.Upsample):\n",
        "                        features_before_up.append(x)\n",
        "                        x = module1(x)\n",
        "                        x = torch.cat([x,before_skip1], dim=1)\n",
        "                        #print(\"Up detected level2\")\n",
        "\n",
        "            if isinstance(module, nn.Upsample): # не зайдет в него\n",
        "                features_before_up.append(x)\n",
        "                x = module(x)\n",
        "                #print(\"Up detected level1\")\n",
        "        #return features_before_up\n",
        "        features = [feature.squeeze().cpu().numpy() for feature in features_before_up]\n",
        "        feature0 = np.array(features[0])\n",
        "        feature1 = np.array(features[1])\n",
        "        feature2 = np.array(features[2])\n",
        "        feature3 = np.array(features[3])\n",
        "        # print(feature0.shape)\n",
        "        # print(feature1.shape)\n",
        "        # print(feature2.shape)\n",
        "        # print(feature3.shape)\n",
        "        # Apply average pooling and reshape\n",
        "        # print(\"avg_pool.shape: \", F.avg_pool2d(torch.tensor(feature0), kernel_size=4).shape)\n",
        "        feature0_pooled = F.avg_pool2d(torch.tensor(feature0), kernel_size=4).squeeze()\n",
        "        feature1_pooled = F.avg_pool2d(torch.tensor(feature1), kernel_size=8).squeeze()\n",
        "        feature2_pooled = F.avg_pool2d(torch.tensor(feature2), kernel_size=16).squeeze()\n",
        "        feature3_pooled = F.avg_pool2d(torch.tensor(feature3), kernel_size=32).squeeze()\n",
        "        # print(feature0_pooled.shape)\n",
        "        # print(feature1_pooled.shape)\n",
        "        # print(feature2_pooled.shape)\n",
        "        # print(feature3_pooled.shape)\n",
        "        feature_map = torch.cat((feature0_pooled, feature1_pooled, feature2_pooled, feature3_pooled), axis =1)\n",
        "        #print(feature_map.shape) # (batch_size, 512)\n",
        "        return feature_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpy3GC7XzC7J"
      },
      "source": [
        "# Define the noise schedule and sampling loop\n",
        "\n",
        "def get_alphas_sigmas(log_snrs):\n",
        "    \"\"\"Returns the scaling factors for the clean image (alpha) and for the\n",
        "    noise (sigma), given the log SNR for a timestep.\"\"\"\n",
        "    return log_snrs.sigmoid().sqrt(), log_snrs.neg().sigmoid().sqrt()\n",
        "\n",
        "def get_ddpm_schedule(t):\n",
        "    \"\"\"Returns log SNRs for the noise schedule from the DDPM paper.\"\"\"\n",
        "    return -torch.special.expm1(1e-4 + 10 * t**2).log()\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, x, steps, eta, classes):\n",
        "    \"\"\"Draws samples from a model given starting noise.\"\"\"\n",
        "    ts = x.new_ones([x.shape[0]])\n",
        "\n",
        "    # Create the noise schedule\n",
        "    t = torch.linspace(1, 0, steps + 1)[:-1]\n",
        "    log_snrs = get_ddpm_schedule(t)\n",
        "    alphas, sigmas = get_alphas_sigmas(log_snrs)\n",
        "\n",
        "    # The sampling loop\n",
        "    for i in trange(steps):\n",
        "\n",
        "        # Get the model output (v, the predicted velocity)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            v = model(x, ts * log_snrs[i], classes).float()\n",
        "\n",
        "        # Predict the noise and the denoised image\n",
        "        pred = x * alphas[i] - v * sigmas[i]\n",
        "        eps = x * sigmas[i] + v * alphas[i]\n",
        "\n",
        "        # If we are not on the last timestep, compute the noisy image for the\n",
        "        # next timestep.\n",
        "        if i < steps - 1:\n",
        "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
        "            # downward according to the amount of additional noise to add\n",
        "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
        "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
        "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
        "            # Recombine the predicted noise and predicted denoised image in the\n",
        "            # correct proportions for the next step\n",
        "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
        "            # Add the correct amount of fresh noise\n",
        "            if eta:\n",
        "                x += torch.randn_like(x) * ddim_sigma\n",
        "    # If we are on the last timestep, output the denoised image\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diff net training"
      ],
      "metadata": {
        "id": "My098Q8nyeKN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYdls2GREKLv"
      },
      "source": [
        "# Visualize the noise schedule\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "t_vis = torch.linspace(0, 1, 1000)\n",
        "log_snrs_vis = get_ddpm_schedule(t_vis)\n",
        "alphas_vis, sigmas_vis = get_alphas_sigmas(log_snrs_vis)\n",
        "\n",
        "print('The noise schedule:')\n",
        "\n",
        "plt.plot(t_vis, alphas_vis, label='alpha (signal level)')\n",
        "plt.plot(t_vis, sigmas_vis, label='sigma (noise level)')\n",
        "plt.legend()\n",
        "plt.xlabel('timestep')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(t_vis, log_snrs_vis, label='log SNR')\n",
        "plt.legend()\n",
        "plt.xlabel('timestep')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMWkwqn6zSZ5"
      },
      "source": [
        "# Prepare the dataset\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5]),\n",
        "])\n",
        "train_set = datasets.CIFAR10('data', train=True, download=True, transform=tf)\n",
        "train_dl = data.DataLoader(train_set, batch_size, shuffle=True,\n",
        "                           num_workers=4, persistent_workers=True, pin_memory=True)\n",
        "val_set = datasets.CIFAR10('data', train=False, download=True, transform=tf)\n",
        "val_dl = data.DataLoader(val_set, batch_size,\n",
        "                         num_workers=4, persistent_workers=True, pin_memory=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egOFiEQ_zL25"
      },
      "source": [
        "# Create the model and optimizer\n",
        "\n",
        "seed = 0\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model = Diffusion().to(device)\n",
        "model_ema = deepcopy(model)\n",
        "print('Model parameters:', sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "opt = optim.Adam(model.parameters(), lr=2e-4)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "epoch = 0\n",
        "\n",
        "# Use a low discrepancy quasi-random sequence to sample uniformly distributed\n",
        "# timesteps. This considerably reduces the between-batch variance of the loss.\n",
        "rng = torch.quasirandom.SobolEngine(1, scramble=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blNYA6yzzuXY"
      },
      "source": [
        "# Actually train the model\n",
        "\n",
        "ema_decay = 0.998\n",
        "\n",
        "# The number of timesteps to use when sampling\n",
        "steps = 500\n",
        "\n",
        "# The amount of noise to add each timestep when sampling\n",
        "# 0 = no noise (DDIM)\n",
        "# 1 = full noise (DDPM)\n",
        "eta = 1.\n",
        "\n",
        "\n",
        "def eval_loss(model, rng, reals, classes):\n",
        "    # Draw uniformly distributed continuous timesteps\n",
        "    t = rng.draw(reals.shape[0])[:, 0].to(device)\n",
        "\n",
        "    # Calculate the noise schedule parameters for those timesteps\n",
        "    log_snrs = get_ddpm_schedule(t)\n",
        "    alphas, sigmas = get_alphas_sigmas(log_snrs)\n",
        "    weights = log_snrs.exp() / log_snrs.exp().add(1)\n",
        "\n",
        "    # Combine the ground truth images and the noise\n",
        "    alphas = alphas[:, None, None, None]\n",
        "    sigmas = sigmas[:, None, None, None]\n",
        "    noise = torch.randn_like(reals)\n",
        "    noised_reals = reals * alphas + noise * sigmas\n",
        "    targets = noise * alphas - reals * sigmas\n",
        "\n",
        "    # Compute the model output and the loss.\n",
        "    with torch.cuda.amp.autocast():\n",
        "        v = model(noised_reals, log_snrs, classes)\n",
        "        return (v - targets).pow(2).mean([1, 2, 3]).mul(weights).mean()\n",
        "\n",
        "\n",
        "def train():\n",
        "    for i, (reals, classes) in enumerate(tqdm(train_dl)):\n",
        "        opt.zero_grad()\n",
        "        reals = reals.to(device)\n",
        "        classes = classes.to(device)\n",
        "\n",
        "        # Evaluate the loss\n",
        "        loss = eval_loss(model, rng, reals, classes)\n",
        "\n",
        "        # Do the optimizer step and EMA update\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        ema_update(model, model_ema, 0.95 if epoch < 20 else ema_decay)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            tqdm.write(f'Epoch: {epoch}, iteration: {i}, loss: {loss.item():g}')\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "@torch.random.fork_rng()\n",
        "@eval_mode(model_ema)\n",
        "def val():\n",
        "    tqdm.write('\\nValidating...')\n",
        "    torch.manual_seed(seed)\n",
        "    rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "    for i, (reals, classes) in enumerate(tqdm(val_dl)):\n",
        "        reals = reals.to(device)\n",
        "        classes = classes.to(device)\n",
        "\n",
        "        loss = eval_loss(model_ema, rng, reals, classes)\n",
        "\n",
        "        total_loss += loss.item() * len(reals)\n",
        "        count += len(reals)\n",
        "    loss = total_loss / count\n",
        "    tqdm.write(f'Validation: Epoch: {epoch}, loss: {loss:g}')\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "@torch.random.fork_rng()\n",
        "@eval_mode(model_ema)\n",
        "def demo():\n",
        "    tqdm.write('\\nSampling...')\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    noise = torch.randn([100, 3, 32, 32], device=device)\n",
        "    fakes_classes = torch.arange(10, device=device).repeat_interleave(10, 0)\n",
        "    fakes = sample(model_ema, noise, steps, eta, fakes_classes)\n",
        "\n",
        "    grid = utils.make_grid(fakes, 10).cpu()\n",
        "    filename = f'demo_{epoch:05}.png'\n",
        "    TF.to_pil_image(grid.add(1).div(2).clamp(0, 1)).save(filename)\n",
        "    display.display(display.Image(filename))\n",
        "    tqdm.write('')\n",
        "\n",
        "\n",
        "def save():\n",
        "    filename = 'cifar_diffusion.pth'\n",
        "    obj = {\n",
        "        'model': model.state_dict(),\n",
        "        'model_ema': model_ema.state_dict(),\n",
        "        'opt': opt.state_dict(),\n",
        "        'scaler': scaler.state_dict(),\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    torch.save(obj, filename)\n",
        "\n",
        "\n",
        "try:\n",
        "    val()\n",
        "    demo()\n",
        "    while True:\n",
        "        print('Epoch', epoch)\n",
        "        train()\n",
        "        epoch += 1\n",
        "        if epoch % 5 == 0:\n",
        "            val()\n",
        "            demo()\n",
        "        save()\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WMg9EYCN3Ehy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source_path = '/content/cifar_diffusion.pth'\n",
        "destination_path = '/content/drive/My Drive/without_labels_cifar_diffusion_30_epochs.pth'\n",
        "\n",
        "shutil.copyfile(source_path, destination_path)\n"
      ],
      "metadata": {
        "id": "63p0DGY3zgPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model"
      ],
      "metadata": {
        "id": "8fndPVr2Yxky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "y9T4VhbTcvVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "weights_path = '/content/drive/My Drive/without_labels_cifar_diffusion_30_epochs.pth' # Path to the weights file on Google Drive\n",
        "# '/content/drive/My Drive/cifar_diffusion_30_epochs.pth'\n",
        "#or '/content/drive/My Drive/cifar_diffusion.pth'\n",
        "\n",
        "saved_obj = torch.load(weights_path, map_location=torch.device('cpu')) # Load the weights + some info about trainig\n",
        "model_dif = Diffusion().to(device) # Create an instance of your Diffusion model\n",
        "model_dif.load_state_dict(saved_obj['model']) # Load the weights into the model"
      ],
      "metadata": {
        "id": "xgL4BfjAYzrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My 1 image demonstration"
      ],
      "metadata": {
        "id": "6VhMMU5FO97V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "ema_decay = 0.998\n",
        "steps = 500\n",
        "eta = 1.\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "model_ema = deepcopy(model_dif)\n",
        "print('Model parameters:', sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "id": "0TG4cR6ihAN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "@torch.random.fork_rng()\n",
        "@eval_mode(model_ema)\n",
        "def demo_small():\n",
        "    tqdm.write('\\nSampling...')\n",
        "    torch.manual_seed(seed+100)\n",
        "    noise = torch.randn([1, 3, 32, 32], device=device)\n",
        "    #fakes_classes = torch.arange(1, device=device).repeat_interleave(1, 0)\n",
        "    fakes_classes = torch.tensor([1])\n",
        "    print(fakes_classes)\n",
        "    fakes = sample(model_ema, noise, steps, eta, fakes_classes)\n",
        "    print(fakes.shape)\n",
        "    fakes = fakes.squeeze().permute(1, 2, 0).cpu().numpy()\n",
        "    print(fakes.shape)\n",
        "    plt.imshow(fakes)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ugdsvAtHPCEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_small()"
      ],
      "metadata": {
        "id": "hSYdd23iPDPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature extraction example"
      ],
      "metadata": {
        "id": "fnEpV9t5krpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "import numpy as np\n",
        "rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # need to add normalization\n",
        "])\n",
        "\n",
        "\n",
        "dataset = CIFAR10(root='./data', train=False, download=True, transform=transform) # Load the CIFAR10 dataset\n",
        "dataloader_small = DataLoader(dataset, batch_size=10, shuffle=False)\n",
        "model_dif.eval() # Set the model to evaluation mode\n",
        "\n",
        "c = 0\n",
        "for image, classes in dataloader_small:\n",
        "    print(image.shape)\n",
        "    print(classes)\n",
        "    t = rng.draw(image.shape[0])[:, 0]\n",
        "    print(t.shape)\n",
        "    log_snrs = get_ddpm_schedule(t) # Calculate the noise schedule parameters for those timesteps\n",
        "    # Compute features for one image\n",
        "    with torch.no_grad():\n",
        "        features = model_dif.get_features(image, log_snrs, classes)\n",
        "        #features = [feature.squeeze().cpu().numpy() for feature in features]\n",
        "        c+=1\n",
        "        if c==1:\n",
        "            break"
      ],
      "metadata": {
        "id": "pJaE3GP1nT_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t"
      ],
      "metadata": {
        "id": "y7nXEwy9zb57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.shape"
      ],
      "metadata": {
        "id": "d_Z2ROMCqNc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Umap feature vizualization"
      ],
      "metadata": {
        "id": "pFaru2PiCkje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install umap-learn"
      ],
      "metadata": {
        "id": "WaVwPdLSCha3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10\n",
        "import numpy as np\n",
        "rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # need to add normalization\n",
        "    transforms.Normalize([0.5], [0.5]),\n",
        "])\n",
        "\n",
        "\n",
        "dataset = CIFAR10(root='./data', train=False, download=True, transform=transform) # Load the CIFAR10 dataset\n",
        "dataloader_small = DataLoader(dataset, batch_size=1000, shuffle=False)\n",
        "model_dif.eval() # Set the model to evaluation mode\n",
        "\n",
        "c = 0\n",
        "for image, classes in dataloader_small:\n",
        "    print(image.shape)\n",
        "    #print(classes)\n",
        "    #t = rng.draw(image.shape[0])[:, 0]\n",
        "    t = torch.tensor([0.001] * 1000)\n",
        "\n",
        "    print(t.shape)\n",
        "    log_snrs = get_ddpm_schedule(t) # Calculate the noise schedule parameters for those timesteps\n",
        "    # Compute features for one batch\n",
        "    with torch.no_grad():\n",
        "        features = model_dif.get_features(image, log_snrs, classes)\n",
        "        # features shape: (num_samples, num_features)\n",
        "        # labels shape: (num_samples,)\n",
        "        scaler = StandardScaler()\n",
        "        normalized_features = scaler.fit_transform(features)\n",
        "        reducer = umap.UMAP()\n",
        "        embedding = reducer.fit_transform(normalized_features)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.scatter(embedding[:, 0], embedding[:, 1], c=classes, cmap='tab10', s=5)\n",
        "        plt.title('UMAP Visualization of CIFAR-10 Features')\n",
        "        plt.colorbar()\n",
        "        plt.show()\n",
        "        c+=1\n",
        "        if c==1:\n",
        "            break"
      ],
      "metadata": {
        "id": "ZENFe9LpB4nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "for image, classes in dataloader_small:\n",
        "    print(image.shape)\n",
        "    #print(classes)\n",
        "    #t = rng.draw(image.shape[0])[:, 0]\n",
        "    t = torch.tensor([0.001] * 1000)\n",
        "\n",
        "    print(t.shape)\n",
        "    log_snrs = get_ddpm_schedule(t) # Calculate the noise schedule parameters for those timesteps\n",
        "    # Compute features for one batch\n",
        "    with torch.no_grad():\n",
        "        features = model_dif.get_features(image, log_snrs, classes)\n",
        "        # features shape: (num_samples, num_features)\n",
        "        # labels shape: (num_samples,)\n",
        "        scaler = StandardScaler()\n",
        "        normalized_features = scaler.fit_transform(features)\n",
        "        # reducer = umap.UMAP()\n",
        "        # embedding = reducer.fit_transform(normalized_features)\n",
        "        umap_3d = umap.UMAP(n_components=3, random_state=42, n_neighbors=10, min_dist=0.3)\n",
        "        projection = umap_3d.fit_transform(normalized_features)\n",
        "        # plt.figure(figsize=(10, 8))\n",
        "        # plt.scatter(embedding[:, 0], embedding[:, 1], c=classes, cmap='tab10', s=5)\n",
        "        # plt.title('UMAP Visualization of CIFAR-10 Features')\n",
        "        # plt.colorbar()\n",
        "        # plt.show()\n",
        "\n",
        "        # Plot the 3D graph\n",
        "        fig = plt.figure(figsize=(10, 7))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "        # Scatter plot with labels as color\n",
        "        scatter = ax.scatter(projection[:, 0], projection[:, 1], projection[:, 2], c=classes, cmap='Spectral', s=5)\n",
        "        legend1 = ax.legend(*scatter.legend_elements(), title=\"Digits\")\n",
        "        ax.add_artist(legend1)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        c+=1\n",
        "        if c==1:\n",
        "            break"
      ],
      "metadata": {
        "id": "EAuT0rsX3sSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward process noising"
      ],
      "metadata": {
        "id": "7RD2VYQaaUce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 1\n",
        "\n",
        "tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5]),\n",
        "])\n",
        "\n",
        "train_dl2 = data.DataLoader(train_set, bs, shuffle=True,\n",
        "                           num_workers=4, persistent_workers=True, pin_memory=True)\n",
        "for i, (reals, classes) in enumerate(tqdm(train_dl2)):\n",
        "    reals = reals.to(device)\n",
        "    classes = classes.to(device)\n",
        "    t_list = [0.001, 0.01, 0.1, 0.5, 0.9]\n",
        "    noised_im = list\n",
        "    for t in t_list:\n",
        "        log_snrs = get_ddpm_schedule(torch.tensor([t]))\n",
        "        alphas, sigmas = get_alphas_sigmas(log_snrs)\n",
        "        weights = log_snrs.exp() / log_snrs.exp().add(1)\n",
        "\n",
        "        # Combine the ground truth images and the noise\n",
        "        alphas = alphas[:, None, None, None]\n",
        "        sigmas = sigmas[:, None, None, None]\n",
        "        noise = torch.randn_like(reals)\n",
        "        noised_reals = reals * alphas + noise * sigmas\n",
        "        print(noised_reals.shape)\n",
        "        noised_reals = noised_reals.squeeze().permute(1, 2, 0).cpu().numpy()\n",
        "        print(noised_reals.shape)\n",
        "        #noised_im.append(noised_reals)\n",
        "        plt.imshow(noised_reals)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    break"
      ],
      "metadata": {
        "id": "oU-6-U-DYADX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Small net train"
      ],
      "metadata": {
        "id": "tNVniM7Vp0uF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(images, labels, t_up, batch_size, model_dif):\n",
        "    \"\"\"\n",
        "    Extracts features, namely concatenates averaged arrays from different layers of the diffusion model with the UNet architecture.\n",
        "    :t_up: from which step of the forward diffusion process image is needed.\n",
        "    :model_dif: a diffusion model.\n",
        "    :forward_diffusion: forward diffusion process.\n",
        "    :return: a tensor of shape [batch_size, 80].\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        t = torch.tensor([t_up] * labels.shape[0])\n",
        "        log_snrs = get_ddpm_schedule(t)\n",
        "        log_snrs = log_snrs.to(device)\n",
        "        alphas, sigmas = get_alphas_sigmas(log_snrs)\n",
        "        weights = log_snrs.exp() / log_snrs.exp().add(1)\n",
        "        alphas = alphas[:, None, None, None]\n",
        "        sigmas = sigmas[:, None, None, None]\n",
        "        noise = torch.randn_like(images)\n",
        "\n",
        "        # print(\"images:\", images.shape)\n",
        "        # print(\"log_snrs:\", log_snrs.shape)\n",
        "        # print(\"labels.shape:\", labels.shape)\n",
        "        # print(\"alphas.shape:\", alphas.shape)\n",
        "        # print(\"sigmas.shape:\", sigmas.shape)\n",
        "        noised_reals = images * alphas + noise * sigmas\n",
        "        # print(noised_reals.shape)\n",
        "        features = model_dif.get_features(noised_reals.to(device), log_snrs, labels)\n",
        "        #print(features.shape)\n",
        "    return torch.tensor(features)"
      ],
      "metadata": {
        "id": "1IptJhjRoIcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, t_up, batch_size, model, criterion, optimizer, model_dif,  epochs=90, loss_list=[]):\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features = extract_features(images, labels, t_up, batch_size, model_dif.to(device)).to(device)\n",
        "            #print(features)\n",
        "            outputs = model(features)\n",
        "            optimizer.zero_grad()\n",
        "            #print(labels)\n",
        "            loss = criterion(outputs, labels)\n",
        "            #print(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        loss_list.append(running_loss / len(train_loader))\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
        "    return loss_list"
      ],
      "metadata": {
        "id": "GZ4NhxpRpuFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader, t_up, batch_size, model_dif=model_dif):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            features = extract_features(images, labels, t_up, batch_size, model_dif).to(device)\n",
        "            outputs = model(features)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy: {accuracy}%\")\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "KCJQXRo6pzdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "\n",
        "tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5]),\n",
        "])\n",
        "\n",
        "train_set = datasets.CIFAR10('data', train=True, download=True, transform=tf)\n",
        "train_dl = data.DataLoader(train_set, batch_size, shuffle=True,\n",
        "                           num_workers=4, persistent_workers=True, pin_memory=True)\n",
        "val_set = datasets.CIFAR10('data', train=False, download=True, transform=tf)\n",
        "val_dl = data.DataLoader(val_set, batch_size,\n",
        "                         num_workers=4, persistent_workers=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "_f1bQDrM0MPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from smallnet import LinearNet, Net, split_dataset\n",
        "input_size = 512  # embedding size\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "t_up = 0.1  # from 0 to 1!!!\n",
        "\n",
        "model = Net(input_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "loss_list = train_model(train_dl, t_up, batch_size, model, criterion, optimizer, model_dif, epochs=epochs)\n",
        "accuracy = test_model(model, val_dl, t_up, batch_size, model_dif=model_dif)\n",
        "accuracy"
      ],
      "metadata": {
        "id": "Lj0G6TWDqmv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = test_model(model, val_dl, t_up, batch_size, model_dif=model_dif)"
      ],
      "metadata": {
        "id": "ijljUJTztqZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBVa5sfx_y5Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}